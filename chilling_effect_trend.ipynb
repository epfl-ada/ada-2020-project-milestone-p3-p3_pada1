{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone P4 - A comparative approach on chilling effect, from Wikipedia to Google Trend\n",
    "\n",
    "## Course CS-401 - Applied Data Analysis\n",
    "\n",
    "### Instructor : [Robert West](https://dlab.epfl.ch/people/west/)\n",
    "\n",
    "### Author :  \n",
    "- **Chraibi Ghali**  \n",
    "    SCIPER: 262251\n",
    "- **Jesslen Artur**  \n",
    "    SCIPER: 270642\n",
    "- **Michels Luc**  \n",
    "    SCIPER: 273666  \n",
    "\n",
    "#### Due date: 18 Dec 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "\n",
    "from os import path\n",
    "import json\n",
    "import requests\n",
    "\n",
    "from helper import *\n",
    "\n",
    "data_path = \"data/\"\n",
    "base_request_prefix = \"https://kgsearch.googleapis.com/v1/entities:search\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Context\n",
    "\n",
    "Lorem ipsum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Scrapping of the data\n",
    "\n",
    "**> Wikipedia to google trends**\n",
    "\n",
    "Using google trends instead of wikipedia has advantages and limitations (that will be discussed in the Discussion section). By using google trends, we can get location specific answers and also use language agnostic requests through topics. However, some limitations were introduced as google trends does not provide absolute article views numbers but instead gives normalized results based on the highest search count in the specified timeframe and location ([2]). In this section, we will explain how we obtained our google trends dataframes.\n",
    "\n",
    "**Step 1: Article to Search term**\n",
    "\n",
    "To reproduce the results obtained by Dr. Jonathon W. Penney ([9]) we first need to translate the terms from their respective DHS categories into google search terms. In most cases we can use the DHS terms directly. However if there are several terms to translate we use the one closest to the wikipedia article chosen by Dr Penney (ex: “Agent/Espionage” becomes “Espionage”) ([9]).  \n",
    "\n",
    "**Step 2: Search term to topic**\n",
    "\n",
    "Topics “are a group of terms that share the same concept in any language” ([4]) . This implies that the same terms but in different languages are also grouped together.\n",
    "\n",
    "Now that we have search terms we want to use the topics they represent. If we directly use the DHS categories terms we will probably not obtain any correct results. Indeed, if we compare the interests for the search terms “Federal Bureau of Investigation” and “FBI” then we will see that the vast majority of internet surfers use “FBI” which should point to the same entity ([6]). Instead we can use freebase IDs which regroups the terms with the same meaning together and corresponds to the “topics” in google trends ([3]()).\n",
    "\n",
    "However, the freebase API is no longer usable ([5]). We have to pass through the Google Knowledge Graph Search API instead. This API lets us search for all entities in the Google Knowledge graph and gives us the entities closest to the search term we input. Each entity has a “resultScore:\tAn indicator of how well the entity matched the request constraints.” ([7]). Then we can parse the best entity for its corresponding freebase id (sometimes GKG id ([1]).\n",
    "However, by choosing the entity with the highest resultScore we had made the assumption none of our DHS terms are ambiguous and the closest match will always be the correct topic. This holds for most DHS terms but is not true for all. We then decided to individually fix those mismatched topics by finding the correct topic on google trends and dropping the term if no such topic existed. Also we found that “terror”  and “terrorism” both point to the \"Terrorism\" entity so we kept only “terrorism”.\n",
    "\n",
    "| Original DHS Term     | Mismatched topic                                  | Replaced topic |\n",
    "| --------------------- | ------------------------------------------------- | ------ |\n",
    "| attack                | Attack on Titan                                   | Attack |\n",
    "| Agro                  | Agronomy                                          | No terrorism topic found |\n",
    "| Nuclear Enrichment    | International Conference on Nuclear Fuel Cycle…   | No terrorism topic found |\n",
    "| Nuclear               | Magnetic resonance imaging                        | Nuclear weapon |\n",
    "| Biological Weapon     | Biological Weapons Convention                     | biological weapon |\n",
    "| FARC                  | Farce                                             | Revolutionary Armed Forces of Colombia—People's Army |\n",
    "| Suicide bomber        | Female suicide bomber                             | No suicide bomber topic found |\n",
    "| Pirates               | Pirates of the Caribbean: The Curse of the Bla…   | No terrorism topic found |\n",
    "| Secret Service        | Kingsman: The Secret Service                      | United States Secret Service |\n",
    "| Agro                  | Agronomy                                          | Agro-terrorism |\n",
    "\n",
    "\n",
    "When using the GTab API (explained in next section), two of our queries returned with “bad response”. We decided to drop these topics as there were no suitable replacement topics.\n",
    "\n",
    "In the end we were missing:\n",
    "\n",
    "Terrorism category ( 6/ 48 ):  \n",
    "Agro  \n",
    "Nuclear Enrichment  \n",
    "Suicide bomber  \n",
    "Pirates  \n",
    "Environmental Terrorism (*Bad response*)  \n",
    "Terror (*Pointed to an already existing Topic*)  \n",
    "\n",
    "Domestic category ( 1/ 25 ):  \n",
    "Secure Border Initiative (*Bad response*)  \n",
    "\n",
    "**Step 3: Google Trends Anchor Bank**\n",
    "\n",
    "Now that we have topics we can directly use it to query any google trends API ([3]). However, as explained above, the results obtained are normalized and we can compare only up to 5 queries ([2]). This means that we cannot compare our results together as we don’t have the absolute values for each query and we have a minimum of 24 topics to compare. \n",
    "\n",
    "We can use the GTab API to bypass this problem. Indeed, GTab creates an offline anchor bank with a wide range of queries and one reference query. All subsequent queries are then compared to the closest stored query with a binary search which is in turn compared to the reference query. Therefore, all queries we make are relative to the reference query which means we can compare our queries together and we also remove rounding errors in the event we would be comparing topics with too large differences in popularity ([8]). \n",
    "\n",
    "**Step 4: Creating the dataframes**\n",
    "\n",
    "We now have all the required tools for creating the dataframes. We start by creating the required anchor banks for GTab. We create two anchor banks with a US search location. \n",
    "Indeed, the original terms for the queries come from the DHS, the Department of Homeland Security. The terms which originate from this institution are mostly directed towards the American population. Most notably, the “domestic” category mainly uses US organisations (ex: DHS, FBI, CIA, etc).\n",
    "Then we chose two timeframes: one for the paper timeframe (2012-01-01 -> 2014-08-31) and one that goes until the end of November 2020 (2012-01-01 -> 2020-11-30). Afterwards, we query google trends using the GTab API and the corresponding anchor bank.\n",
    "\n",
    "The data frame created has following attributes:  \n",
    "\n",
    "| Name | Description |\n",
    "| ----------------- | ---------------- |\n",
    "| Date          | Starting day of the week where the data is aggregated |\n",
    "| max_ratio     | Calibrated value based on the reference query of the anchor bank (most popular query of the anchor bank) |\n",
    "| max_ratio_hi  | Low error bound determined by gtab |\n",
    "| max_ratio_lo  | High error bound determined by gtab |\n",
    "| article_name  | Name of the original DHS term |\n",
    "\n",
    "\n",
    "[1]: https://searchengineland.com/laymans-visual-guide-googles-knowledge-graph-search-api-241935\n",
    "\n",
    "[2]: https://support.google.com/trends/answer/4365533?hl=en\n",
    "\n",
    "[3]: https://trends.google.com/trends/explore?geo=US&q=%2Fm%2F02_1m\n",
    "\n",
    "[4]: https://support.google.com/trends/answer/4359550\n",
    "\n",
    "[5]: https://developers.google.com/freebase/v1/search\n",
    "\n",
    "[6]: https://trends.google.com/trends/explore?geo=US&q=Federal%20Bureau%20of%20Investigation,fbi\n",
    "\n",
    "[7]: https://developers.google.com/knowledge-graph/reference/rest/v1\n",
    "\n",
    "[8]: https://arxiv.org/abs/2007.13861\n",
    "\n",
    "[9]: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2769645"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 0: Verify functioning of the API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAPPING = False\n",
    "if MAPPING: \n",
    "    # Read API_key (you need your own key for it to work)\n",
    "    with open(data_path + \"API_key\",\"r\") as f:\n",
    "        API_key = f.read()\n",
    "    # Create simple request to show structure and test if functional\n",
    "    params = {\n",
    "        \"query\" : \"iraq\",\n",
    "        \"key\"   : API_key,\n",
    "        \"limit\" : 1,  \n",
    "        \"indent\": True\n",
    "    }\n",
    "    r = requests.get(base_request_prefix, params = params)\n",
    "    entity = r.json()\n",
    "    print(f\"JSON structure returned by the API:\\n{get_json_structure(entity)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Article to Search term**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print \n",
    "search_queries = load_pickle(data_path + 'search_queries.pkl')\n",
    "for key, value in search_queries.items():\n",
    "    print(f\"{key}:\\n{value}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Search term to topic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MAPPING:\n",
    "    # Terrorism dataset Mapping\n",
    "    terrorism_mapping_df = create_search_terms_to_GKG_node_df(search_queries['terrorism'], \n",
    "        \"terrorism\", API_key)\n",
    "    top_30_terrorism_mapping_df = create_search_terms_to_GKG_node_df(search_queries['top_30_terrorism'], \n",
    "        \"top_30_terrorism\", API_key)\n",
    "    # Create id replacement dictionary\n",
    "    id_replacement = {\n",
    "        \"/m/0gtxdb2\" : \"/g/11bc5q9v7r\",\n",
    "        \"/m/04xkp\"   : \"/m/05gpf\",\n",
    "        \"/m/01xmw0\"  : \"/g/121x751y\",\n",
    "        \"/m/011ys5\"  : \"/m/06hvg\",\n",
    "        \"/m/019jkv\"  : \"/m/0gggw2\"\n",
    "\n",
    "    }\n",
    "\n",
    "    # Create name replacement dictionary\n",
    "    name_replacement = {\n",
    "        \"Attack on Titan\" : \"Attack\",\n",
    "        \"Magnetic resonance imaging\" : \"Nuclear weapon\",\n",
    "        \"Biological Weapons Convention\" : \"biological weapon\",\n",
    "        \"Farce\" : \"Revolutionary Armed Forces of Colombia—People's Army\",\n",
    "        \"Agronomy\" : \"Agro-terrorism\"\n",
    "\n",
    "    }\n",
    "\n",
    "    # Create list of topics ids to delete\n",
    "    topics_id_to_delete = [\n",
    "        \"/g/11k69f5spb\",\n",
    "        \"/g/11cr_hd3g5\",\n",
    "        \"/m/01vksx\",\n",
    "        \"/m/0jz65\"\n",
    "    ]\n",
    "    # Fix ambiguous topics\n",
    "    terrorism_mapping_df = fix_topics(terrorism_mapping_df, id_replacement, \n",
    "        name_replacement, topics_id_to_delete)\n",
    "    top_30_terrorism_mapping_df = fix_topics(top_30_terrorism_mapping_df, \n",
    "        id_replacement, name_replacement, topics_id_to_delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MAPPING:\n",
    "    domestic_mapping_df = create_search_terms_to_GKG_node_df(search_queries['domestic'], \"domestic\", API_key)\n",
    "    # Create id replacement dictionary\n",
    "    domestic_id_replacement = { \"/m/0y4n5ll\" : \"/m/0fynw\"}\n",
    "    # Create name replacement dictionary\n",
    "    domestic_name_replacement = {\"Kingsman: The Secret Service\" : \"United States Secret Service\"}\n",
    "    # Fix ambiguous topics\n",
    "    domestic_mapping_df = fix_topics(domestic_mapping_df, domestic_id_replacement, domestic_name_replacement, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MAPPING:\n",
    "    all_mappings = [terrorism_mapping_df, domestic_mapping_df, top_30_terrorism_mapping_df]\n",
    "    # Concatenate all mappings into one dataframe\n",
    "    all_mappings_df = pd.concat(all_mappings).reset_index(drop=True)\n",
    "    # Save dataframe to pickle\n",
    "    all_mappings_df.to_pickle(data_path+\"mapping.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3 & 4: Google Trends Anchor Bank and creation of the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(domain_name, geo, t):\n",
    "    \"\"\"\n",
    "    Creates a dataframe concatenating all search queries interest over time data from google\n",
    "    trends for one domain. The search queries will use the corresponding topics to the DHS category for the domain.\n",
    "    Prints parametters of queries who failed.\n",
    "    The returned dataframe has attributes:\n",
    "    {date, max_ratio, max_ratio_hi, max_ratio_l, article_name, topic_name, topic_id, geo}\n",
    "\n",
    "    Args:\n",
    "        domain_name (str): domain of the search queries (ie. terrorims, domestic, top_30_terrorism)       \n",
    "        geo (str): geolocalisation of the search query\n",
    "        t (GTAB): GoogleTrendsAnchorBank to use for the queries it needs to be consistent with the geo parameter\n",
    "\n",
    "    Returns:\n",
    "        dataframe: dataframe concatenating all search queries interest over time data from google trends\n",
    "\n",
    "    \"\"\"\n",
    "    mapping_df = load_pickle(data_path+\"mapping.pkl\")\n",
    "    \n",
    "    # Get the list of the article names of this domain \n",
    "    topic_queries_articles = mapping_df[mapping_df[\"domain_name\"] == domain_name][\"search_term\"].tolist()\n",
    "    # Get the list of topic ids of this domain \n",
    "    topic_queries_ids = mapping_df[mapping_df[\"domain_name\"] == domain_name][\"entity_id\"].tolist()\n",
    "    # Get the list of topic names of this domain \n",
    "    topic_queries_name = mapping_df[mapping_df[\"domain_name\"] == domain_name][\"entity_name\"].tolist()\n",
    "    \n",
    "    # For each search query freebase id create the corresponding interest over time google trends data\n",
    "    all_interest_over_time_dfs = [t.new_query(search_query) for search_query in topic_queries_ids]\n",
    "    \n",
    "    # Find all queries who succeeded                 \n",
    "    successful_queries = [type(df) != type(-1) for df in all_interest_over_time_dfs]\n",
    "    \n",
    "    df_to_concatenate = []\n",
    "    # Append the name and location to all dataframes\n",
    "    for i, df in enumerate(all_interest_over_time_dfs):\n",
    "        \n",
    "        # Was it a successful query\n",
    "        if successful_queries[i]:\n",
    "            \n",
    "            # Add the article name collumn \n",
    "            df[\"article_name\"] = [topic_queries_articles[i]]*len(df)\n",
    "            # Add the topic name collumn \n",
    "            df[\"topic_id\"] = [topic_queries_ids[i]]*len(df)\n",
    "            # Add the topic id collumn \n",
    "            df[\"topic_name\"] = [topic_queries_name[i]]*len(df)\n",
    "            # Add the localisation collumn\n",
    "            df[\"geo\"] = [\"worldwide\" if geo == \"\" else geo]*len(df)\n",
    "            \n",
    "            df_to_concatenate.append(df)\n",
    "            \n",
    "        # If not we print the parametters that failed\n",
    "        else:\n",
    "            print(\"Found a failed query\")\n",
    "            print(f\"Article name: {topic_queries_articles[i]}\")\n",
    "            print(f\"Topic name: {topic_queries_name[i]}\")\n",
    "            print(f\"Topic id: {topic_queries_ids[i]}\")\n",
    "    \n",
    "    # Concatenate all dfs into one centrale one\n",
    "    return pd.concat(df_to_concatenate).reset_index()\n",
    "    \n",
    "def get_topics(domain_name, data_path):\n",
    "    \"\"\"Gives the list of all topics corresponding to the search queries for the specific domain.\n",
    "    \n",
    "    Args:\n",
    "         domain_name (str): domain name of the search queries (i.e: terrorism, domestic, top_30_terrorism)\n",
    "         data_path (str): directory path containing the mapping pickle file \n",
    "    \n",
    "    Returns:\n",
    "        list of all topics for this domain\n",
    "    \"\"\"\n",
    "    mapping_df = load_pickle(data_path+\"mapping.pkl\")\n",
    "    \n",
    "    return mapping_df[mapping_df[\"domain_name\"] == domain_name][\"entity_id\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE_DATAFRAMES = False\n",
    "if CREATE_DATAFRAMES:\n",
    "    # We choose the US geolocalisation because categories are from the DHS\n",
    "    for geo in [\"US\", \"CA\", \"GB\", \"AU\", \"CH\"]:\n",
    "\n",
    "        # Create time frame corresponding to the paper study\n",
    "        start_timeframe = \"2012-01-01\"\n",
    "        end_timeframe   = \"2014-08-31\"\n",
    "\n",
    "        t = create_and_set_gtab(start_timeframe, end_timeframe, geo)\n",
    "\n",
    "        # Create terrorism dataframe\n",
    "        terrorism_df = create_dataframe(\"terrorism\", geo, t)\n",
    "        # Create domestic dataframe\n",
    "        domestic_df = create_dataframe(\"domestic\", geo, t)\n",
    "        # Create top-30 terrorism dataframe\n",
    "        top_30_terrorism_df = create_dataframe(\"top_30_terrorism\", geo, t)\n",
    "        # Save dataframes to pickle\n",
    "        terrorism_df.to_pickle(data_path+f\"terrorism_{geo}.pkl\")\n",
    "        domestic_df.to_pickle(data_path+f\"domestic_{geo}.pkl\")\n",
    "        top_30_terrorism_df.to_pickle(data_path+f\"top_30_terrorism_{geo}.pkl\")\n",
    "        \n",
    "        # ---------------------------------------------------------------\n",
    "\n",
    "        # Set start to paper start timeframe and end to present time (Stop at November 2020)\n",
    "        start_timeframe = \"2012-01-01\"\n",
    "        end_timeframe   = \"2020-11-30\"\n",
    "\n",
    "        t = create_and_set_gtab(start_timeframe, end_timeframe, geo)\n",
    "\n",
    "        # Create terrorism dataframe\n",
    "        terrorism_df = create_dataframe(\"terrorism\", geo, t)\n",
    "        # Create domestic dataframe\n",
    "        domestic_df = create_dataframe(\"domestic\", geo, t)\n",
    "        # Create top-30 terrorism dataframe\n",
    "        top_30_terrorism_df = create_dataframe(\"top_30_terrorism\", geo, t)\n",
    "        # Save dataframe to pickle\n",
    "        terrorism_df.to_pickle(data_path+f\"terrorism_present_{geo}.pkl\")\n",
    "        domestic_df.to_pickle(data_path+f\"domestic_present_{geo}.pkl\")\n",
    "        top_30_terrorism_df.to_pickle(data_path+f\"top_30_terrorism_present_{geo}.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Replicate the experiment of the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/'\n",
    "\n",
    "if path.exists(data_path):\n",
    "    terrorism_df = load_pickle(data_path+\"terrorism_US.pkl\")\n",
    "    domestic_df = load_pickle(data_path+\"domestic_US.pkl\")\n",
    "    terrorism30_df = load_pickle(data_path+\"top_30_terrorism_US.pkl\")\n",
    "else:\n",
    "    raise NotADirectoryError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terrorism_df = terrorism_df[['date', 'max_ratio', 'topic_name']]\n",
    "terrorism30_df = terrorism30_df[['date', 'max_ratio', 'topic_name']]\n",
    "domestic_df = domestic_df[['date', 'max_ratio', 'topic_name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(terrorism_df.shape)\n",
    "terrorism_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(terrorism30_df.shape)\n",
    "terrorism_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(domestic_df.shape)\n",
    "terrorism_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a constant for the date of revelation\n",
    "revelation_date = pd.Timestamp('2013-06-15')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#domestic_df = domestic_df[~(domestic_df['topic_name']== 'Federal Bureau of Investigation')]\n",
    "#set(domestic_df['topic_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organise the data in a time series with the ratio of total views per week\n",
    "terrorism_ts = terrorism_df.groupby(by='date').sum()\n",
    "terrorism_ts = terrorism_ts.reset_index().rename(columns={'index': 'date'})\n",
    "\n",
    "# Separate the data in two to perform an ITS analysis\n",
    "terrorism_pre_june_ts = terrorism_ts[terrorism_ts['date'] < revelation_date]\n",
    "terrorism_post_june_ts = terrorism_ts[terrorism_ts['date'] > revelation_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_pre_june = np.mean(terrorism_pre_june_ts['max_ratio'])\n",
    "mean_post_june = np.mean(terrorism_post_june_ts['max_ratio'])\n",
    "plt.bar([0, 1], [mean_pre_june, mean_post_june], color='gray', width=0.4)\n",
    "plt.xticks([0, 1], ['Pre June, 2013', 'Post June, 2013'])\n",
    "plt.ylabel(\"Average interest\")\n",
    "plt.xlim(-0.5, 1.5)\n",
    "plt.ylim(0, 200)\n",
    "plt.text(-0.1, mean_pre_june + 5, f\"{mean_pre_june:.2f}\")\n",
    "plt.text(0.9, mean_post_june + 5, f\"{mean_post_june:.2f}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2.1 - Show regression for terrorism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO Modify description of the plot\n",
    "\n",
    "# Visualisation of full time series\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.set_title('Figure 2. Pre and Post June 2013 Topic Ratio Trends \\n\\n', fontsize=14)\n",
    "ax.set_xlabel('\\nTime (?Dates?)', fontsize=11)\n",
    "ax.set_ylabel('Total views (All ?48? Google Articles)\\n', fontsize=11)\n",
    "ax.set_ylim(0, 300)\n",
    "\n",
    "ax.scatter(terrorism_ts.date, terrorism_ts['max_ratio'], \n",
    "    label='Total Articles Views (By Month)', color='black')\n",
    "\n",
    "# Draw the regressions\n",
    "slope_pre_june, intercept_pre_june, _, _, _ = stats.linregress(terrorism_pre_june_ts.index, \n",
    "                                                               terrorism_pre_june_ts['max_ratio'])\n",
    "slope_post_june, intercept_post_june, _, _, _ = stats.linregress(terrorism_post_june_ts.index,\n",
    "                                                                 terrorism_post_june_ts['max_ratio'])\n",
    "x1 = terrorism_pre_june_ts.index\n",
    "ax.plot(terrorism_pre_june_ts.date, intercept_pre_june + slope_pre_june*x1, \n",
    "    color='black', linewidth='3', label='Trend Pre-June 2013')\n",
    "x2 = terrorism_post_june_ts.index\n",
    "ax.plot(terrorism_post_june_ts.date, intercept_post_june + slope_post_june*x2, \n",
    "    color='gray', linewidth='3', label='Trend Post-June 2013')\n",
    "\n",
    "# Emphasise when the revelations occured \n",
    "ax.axvline(x=revelation_date, ymin=0, ymax=280, color='black', linewidth='3')\n",
    "\n",
    "x_text_offset = revelation_date - pd.Timedelta(50, 'd')\n",
    "y_text_offset = 305\n",
    "ax.text(x_text_offset, y_text_offset, 'Mid June 2013', fontsize=10)\n",
    "\n",
    "ax.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2.2 - Remove outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the graph above contains some outliers that may alter our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(5, 1, figsize=(10, 18), sharey=True)\n",
    "\n",
    "points_per_topic = len(terrorism_df)/len(set(terrorism_df['topic_name']))\n",
    "topics_per_plot = 10\n",
    "points_per_plot = points_per_topic * topics_per_plot\n",
    "\n",
    "for i in range(5):\n",
    "    sn.lineplot(data=terrorism_df[i*int(points_per_plot):(i+1)*int(points_per_plot)], \n",
    "                x='date', \n",
    "                y='max_ratio', \n",
    "                hue='topic_name', \n",
    "                ax=ax[i])\n",
    "    ax[i].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terrorism_df = terrorism_df[~(terrorism_df['topic_name']== 'Iraq')]\n",
    "terrorism_df = terrorism_df[~(terrorism_df['topic_name']== 'Terrorism')]\n",
    "terrorism_df = terrorism_df[~(terrorism_df['topic_name']== 'Nigeria')]\n",
    "terrorism_df = terrorism_df[~(terrorism_df['topic_name']== 'Nuclear weapon')]\n",
    "terrorism_df = terrorism_df[~(terrorism_df['topic_name']== 'Hamas')]\n",
    "terrorism_df = terrorism_df[~(terrorism_df['topic_name']== 'Chemical weapon')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organise the data in a time series with the ratio of total views per week\n",
    "terrorism_ts = terrorism_df.groupby(by='date').sum()\n",
    "terrorism_ts = terrorism_ts.reset_index().rename(columns={'index': 'date'})\n",
    "\n",
    "# Separate the data in two to perform an ITS analysis\n",
    "terrorism_pre_june_ts = terrorism_ts[terrorism_ts['date'] < revelation_date]\n",
    "terrorism_post_june_ts = terrorism_ts[terrorism_ts['date'] > revelation_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_pre_june = np.mean(terrorism_pre_june_ts['max_ratio'])\n",
    "mean_post_june = np.mean(terrorism_post_june_ts['max_ratio'])\n",
    "plt.bar([0, 1], [mean_pre_june, mean_post_june], color='gray', width=0.4)\n",
    "plt.xticks([0, 1], ['Pre June, 2013', 'Post June, 2013'])\n",
    "plt.ylabel(\"Average interest\")\n",
    "plt.xlim(-0.5, 1.5)\n",
    "plt.ylim(0, 200)\n",
    "plt.text(-0.1, mean_pre_june + 5, f\"{mean_pre_june:.2f}\")\n",
    "plt.text(0.9, mean_post_june + 5, f\"{mean_post_june:.2f}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO Modify description of the plot\n",
    "\n",
    "# Visualisation of full time series\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.set_title('Figure 2. Pre and Post June 2013 Topic Ratio Trends \\n\\n', fontsize=14)\n",
    "ax.set_xlabel('\\nTime (?Dates?)', fontsize=11)\n",
    "ax.set_ylabel('Total views (All ?48? Google Articles)\\n', fontsize=11)\n",
    "ax.set_ylim(0, 300)\n",
    "\n",
    "ax.scatter(terrorism_ts.date, terrorism_ts['max_ratio'], \n",
    "    label='Total Articles Views (By Month)', color='black')\n",
    "\n",
    "# Draw the regressions\n",
    "slope_pre_june, intercept_pre_june, _, _, _ = stats.linregress(terrorism_pre_june_ts.index, \n",
    "                                                               terrorism_pre_june_ts['max_ratio'])\n",
    "slope_post_june, intercept_post_june, _, _, _ = stats.linregress(terrorism_post_june_ts.index,\n",
    "                                                                 terrorism_post_june_ts['max_ratio'])\n",
    "x1 = terrorism_pre_june_ts.index\n",
    "ax.plot(terrorism_pre_june_ts.date, intercept_pre_june + slope_pre_june*x1, \n",
    "    color='black', linewidth='3', label='Trend Pre-June 2013')\n",
    "x2 = terrorism_post_june_ts.index\n",
    "ax.plot(terrorism_post_june_ts.date, intercept_post_june + slope_post_june*x2, \n",
    "    color='gray', linewidth='3', label='Trend Post-June 2013')\n",
    "\n",
    "# Emphasise when the revelations occured \n",
    "ax.axvline(x=revelation_date, ymin=0, ymax=280, color='black', linewidth='3')\n",
    "\n",
    "x_text_offset = revelation_date - pd.Timedelta(50, 'd')\n",
    "y_text_offset = 305\n",
    "ax.text(x_text_offset, y_text_offset, 'Mid June 2013', fontsize=10)\n",
    "\n",
    "ax.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3.1 - Compare with dommestic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organise the data in time series with the ratio of total views per week\n",
    "terrorism30_ts = terrorism30_df.groupby(by='date').sum()\n",
    "terrorism30_ts = terrorism30_ts.reset_index().rename(columns={'index': 'date'})\n",
    "\n",
    "domestic_ts = domestic_df.groupby(by='date').sum()\n",
    "domestic_ts = domestic_ts.reset_index().rename(columns={'index': 'date'})\n",
    "\n",
    "# Separate the data in two to perform an ITS analysis\n",
    "terrorism30_pre_june_ts = terrorism30_ts[terrorism30_ts['date'] < revelation_date]\n",
    "terrorism30_post_june_ts = terrorism30_ts[terrorism30_ts['date'] > revelation_date]\n",
    "\n",
    "domestic_pre_june_ts = domestic_ts[domestic_ts['date'] < revelation_date]\n",
    "domestic_post_june_ts = domestic_ts[domestic_ts['date'] > revelation_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Show the final figure with both terrorism articles study group and the comparator group\n",
    "## Visualisation of the full time series\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "## Draw the data points, regressions & CIs\n",
    "# on terrorism data (pre/post june)\n",
    "x1 = terrorism30_pre_june_ts.index\n",
    "y1 = terrorism30_pre_june_ts['max_ratio']\n",
    "sn.regplot(x=x1, y=y1, data=terrorism30_pre_june_ts, color=\"black\",\n",
    "           label=\"Terrorism Article Trend Pre-June\", scatter=True, n_boot=10000, ax=ax)\n",
    "\n",
    "x2 = terrorism30_post_june_ts.index\n",
    "y2 = terrorism30_post_june_ts['max_ratio']\n",
    "sn.regplot(x=x2, y=y2, data=terrorism30_post_june_ts, color=\"black\",\n",
    "           label=\"Terrorism Article Trend Post-June\", scatter=True, n_boot=10000, ax=ax)\n",
    "\n",
    "# on domestic security data (pre/post june)\n",
    "x1 = domestic_pre_june_ts.index\n",
    "y1 = domestic_pre_june_ts['max_ratio']\n",
    "sn.regplot(x=x1, y=y1, data=domestic_pre_june_ts, color=\"darkblue\",\n",
    "           label=\"Security Article Trend Pre-June\", scatter=True, n_boot=10000, ax=ax)\n",
    "\n",
    "x2 = domestic_post_june_ts.index\n",
    "y2 = domestic_post_june_ts['max_ratio']\n",
    "sn.regplot(x=x2, y=y2, data=domestic_post_june_ts, color=\"blue\",\n",
    "           label=\"Security Article Trend Post-June\", scatter=True, n_boot=10000, ax=ax)\n",
    "\n",
    "# Emphasise when the revelations occured \n",
    "revelation_date_integer = 75.5\n",
    "ax.axvline(x=revelation_date_integer, ymin=0, ymax=160, color='black', linewidth='3')\n",
    "\n",
    "x_text_offset = revelation_date_integer - 2\n",
    "y_text_offset = 180\n",
    "ax.text(x_text_offset, y_text_offset, 'Mid June 2013', fontsize=10)\n",
    "\n",
    "ax.collections[3].set_label('Confidence interval (95%)')\n",
    "ax.collections[7].set_label('Confidence interval (95%)')\n",
    "\n",
    "ax.set_title('Figure 4A. Terrorism Articles Study Group vs. Domestic Security Comparator Group\\n\\n', fontsize=14)\n",
    "ax.set_xlabel('\\nTime (Months)', fontsize=11)\n",
    "ax.set_ylabel('Total views (All Articles)\\n', fontsize=11)\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3.2 - Look for outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 1, figsize=(10, 14), sharey=True)\n",
    "\n",
    "points_per_topic = len(terrorism30_df)/len(set(terrorism30_df['topic_name']))\n",
    "topics_per_plot = 10\n",
    "points_per_plot = points_per_topic * topics_per_plot\n",
    "\n",
    "for i in range(3):\n",
    "    sn.lineplot(data=terrorism30_df[i*int(points_per_plot):(i+1)*int(points_per_plot)], \n",
    "                x='date', \n",
    "                y='max_ratio', \n",
    "                hue='topic_name', \n",
    "                ax=ax[i])\n",
    "    ax[i].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(terrorism30_df['topic_name']))//topics_per_plot + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 1, figsize=(10, 14), sharey=True)\n",
    "\n",
    "points_per_topic = len(domestic_df)/len(set(domestic_df['topic_name']))\n",
    "topics_per_plot = 10\n",
    "points_per_plot = points_per_topic * topics_per_plot\n",
    "\n",
    "for i in range(3):\n",
    "    sn.lineplot(data=domestic_df[i*int(points_per_plot):(i+1)*int(points_per_plot)], \n",
    "                x='date', \n",
    "                y='max_ratio', \n",
    "                hue='topic_name', \n",
    "                ax=ax[i])\n",
    "    ax[i].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terrorism30_df = terrorism30_df[~(terrorism30_df['topic_name']== 'Terrorism')]\n",
    "terrorism30_df = terrorism30_df[~(terrorism30_df['topic_name']== 'Nuclear weapon')]\n",
    "terrorism30_df = terrorism30_df[~(terrorism30_df['topic_name']== 'Chemical weapon')]\n",
    "\n",
    "domestic_df = domestic_df[~(domestic_df['topic_name']== 'Federal Bureau of Investigation')]\n",
    "domestic_df = domestic_df[~(domestic_df['topic_name']== 'Federal Emergency Management Agency')]\n",
    "domestic_df = domestic_df[~(domestic_df['topic_name']== 'United States Secret Service')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organise the data in time series with the ratio of total views per week\n",
    "terrorism30_ts = terrorism30_df.groupby(by='date').sum()\n",
    "terrorism30_ts = terrorism30_ts.reset_index().rename(columns={'index': 'date'})\n",
    "\n",
    "domestic_ts = domestic_df.groupby(by='date').sum()\n",
    "domestic_ts = domestic_ts.reset_index().rename(columns={'index': 'date'})\n",
    "\n",
    "# Separate the data in two to perform an ITS analysis\n",
    "terrorism30_pre_june_ts = terrorism30_ts[terrorism30_ts['date'] < revelation_date]\n",
    "terrorism30_post_june_ts = terrorism30_ts[terrorism30_ts['date'] > revelation_date]\n",
    "\n",
    "domestic_pre_june_ts = domestic_ts[domestic_ts['date'] < revelation_date]\n",
    "domestic_post_june_ts = domestic_ts[domestic_ts['date'] > revelation_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Show the final figure with both terrorism articles study group and the comparator group\n",
    "## Visualisation of the full time series\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "## Draw the data points, regressions & CIs\n",
    "# on terrorism data (pre/post june)\n",
    "x1 = terrorism30_pre_june_ts.index\n",
    "y1 = terrorism30_pre_june_ts['max_ratio']\n",
    "sn.regplot(x=x1, y=y1, data=terrorism30_pre_june_ts, color=\"black\",\n",
    "           label=\"Terrorism Article Trend Pre-June\", scatter=True, n_boot=10000, ax=ax)\n",
    "\n",
    "x2 = terrorism30_post_june_ts.index\n",
    "y2 = terrorism30_post_june_ts['max_ratio']\n",
    "sn.regplot(x=x2, y=y2, data=terrorism30_post_june_ts, color=\"black\",\n",
    "           label=\"Terrorism Article Trend Post-June\", scatter=True, n_boot=10000, ax=ax)\n",
    "\n",
    "# on domestic security data (pre/post june)\n",
    "x1 = domestic_pre_june_ts.index\n",
    "y1 = domestic_pre_june_ts['max_ratio']\n",
    "sn.regplot(x=x1, y=y1, data=domestic_pre_june_ts, color=\"darkblue\",\n",
    "           label=\"Security Article Trend Pre-June\", scatter=True, n_boot=10000, ax=ax)\n",
    "\n",
    "x2 = domestic_post_june_ts.index\n",
    "y2 = domestic_post_june_ts['max_ratio']\n",
    "sn.regplot(x=x2, y=y2, data=domestic_post_june_ts, color=\"blue\",\n",
    "           label=\"Security Article Trend Post-June\", scatter=True, n_boot=10000, ax=ax)\n",
    "\n",
    "# Emphasise when the revelations occured \n",
    "revelation_date_integer = 75.5\n",
    "ax.axvline(x=revelation_date_integer, ymin=0, ymax=90, color='black', linewidth='3')\n",
    "\n",
    "x_text_offset = revelation_date_integer - 8\n",
    "y_text_offset = 95\n",
    "ax.text(x_text_offset, y_text_offset, 'Mid June 2013', fontsize=10)\n",
    "\n",
    "ax.collections[3].set_label('Confidence interval (95%)')\n",
    "ax.collections[7].set_label('Confidence interval (95%)')\n",
    "\n",
    "ax.set_title('Figure 4A. Terrorism Articles Study Group vs. Domestic Security Comparator Group\\n\\n', fontsize=14)\n",
    "ax.set_xlabel('\\nTime (Months)', fontsize=11)\n",
    "ax.set_ylabel('Total views (All Articles)\\n', fontsize=11)\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "### Discussion:  \n",
    "#### Limitations of our approach: \n",
    "\n",
    "1. Locality and different reference point:  \n",
    "GTab still has some limitations. Indeed, we can only compare queries as long as they use the same anchorbank. If we want to query google trends for different countries we have to use different anchor banks. This means that we cannot quantitatively compare results for different countries. However, we can still offer qualitative insight based on our results. For instance positive trends will stay positive with any reference point."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}