{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone P4 - A comparative approach on chilling effect, from Wikipedia to Google Trend\n",
    "\n",
    "## Course CS-401 - Applied Data Analysis\n",
    "\n",
    "### Instructor : [Robert West](https://dlab.epfl.ch/people/west/)\n",
    "\n",
    "### Author :  \n",
    "- **Chraibi Ghali**  \n",
    "    SCIPER: 262251\n",
    "- **Jesslen Artur**  \n",
    "    SCIPER: 270642\n",
    "- **Michels Luc**  \n",
    "    SCIPER: 273666  \n",
    "\n",
    "#### Due date: 18 Dec 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pickle5\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle5 as pickle\n",
    "\n",
    "from os import path\n",
    "import json\n",
    "import requests\n",
    "\n",
    "from helper import create_and_set_gtab, create_search_terms_to_GKG_node_df, fix_topics, get_json_structure\n",
    "\n",
    "data_path = \"data/\"\n",
    "base_request_prefix = \"https://kgsearch.googleapis.com/v1/entities:search\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Context\n",
    "Lorem ipsum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Scrapping of the data\n",
    "Lorem ipsum"
   ]
  },
  {
   "source": [
    "### Step 1: Create mapping from search queries to Google Knowledge Graph Search Node"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAPPING = False\n",
    "if MAPPING:\n",
    "    # Read API_key (you need your own key for it to work)\n",
    "    with open(data_path+\"API_key\",\"r\") as f:\n",
    "        API_key = f.read()\n",
    "    # Create simple request to show structure and test if functional\n",
    "    params = {\n",
    "        \"query\" : \"iraq\",\n",
    "        \"key\"   : API_key,\n",
    "        \"limit\" : 1,  \n",
    "        \"indent\": True\n",
    "    }\n",
    "    r = requests.get(base_request_prefix, params = params)\n",
    "    entity = r.json()\n",
    "    print(f\"JSON structure returned by the API:\\n{get_json_structure(entity)}\")\n",
    "\n",
    "with open('data/search_queries.pkl', 'rb') as f:\n",
    "    # store the data as binary data stream\n",
    "    search_queries = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MAPPING:\n",
    "    # Terrorism dataset Mapping\n",
    "    terrorism_mapping_df = create_search_terms_to_GKG_node_df(search_queries['terrorism'], \n",
    "        \"terrorism\", API_key)\n",
    "    top_30_terrorism_mapping_df = create_search_terms_to_GKG_node_df(search_queries['top_30_terrorism'], \n",
    "        \"top_30_terrorism\", API_key)\n",
    "    # Create id replacement dictionary\n",
    "    id_replacement = {\n",
    "        \"/m/0gtxdb2\" : \"/g/11bc5q9v7r\",\n",
    "        \"/m/04xkp\"   : \"/m/05gpf\",\n",
    "        \"/m/01xmw0\"  : \"/g/121x751y\",\n",
    "        \"/m/011ys5\"  : \"/m/06hvg\"\n",
    "    }\n",
    "\n",
    "    # Create name replacement dictionary\n",
    "    name_replacement = {\n",
    "        \"Attack on Titan\" : \"Attack\",\n",
    "        \"Magnetic resonance imaging\" : \"Nuclear weapon\",\n",
    "        \"Biological Weapons Convention\" : \"biological weapon\",\n",
    "        \"Farce\" : \"Revolutionary Armed Forces of Colombiaâ€”People's Army\"\n",
    "    }\n",
    "\n",
    "    # Create list of topics ids to delete\n",
    "    topics_id_to_delete = [\n",
    "        \"/g/11k69f5spb\",\n",
    "        \"/g/11cr_hd3g5\",\n",
    "        \"/m/01vksx\"\n",
    "    ]\n",
    "    # Fix ambiguous topics\n",
    "    terrorism_mapping_df = fix_topics(terrorism_mapping_df, id_replacement, \n",
    "        name_replacement, topics_id_to_delete)\n",
    "    top_30_terrorism_mapping_df = fix_topics(top_30_terrorism_mapping_df, \n",
    "        id_replacement, name_replacement, topics_id_to_delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MAPPING:\n",
    "    domestic_mapping_df = create_search_terms_to_GKG_node_df(search_queries['domestic'], \"domestic\", API_key)\n",
    "    # Create id replacement dictionary\n",
    "    domestic_id_replacement = { \"/m/0y4n5ll\" : \"/m/0fynw\"}\n",
    "    # Create name replacement dictionary\n",
    "    domestic_name_replacement = {\"Kingsman: The Secret Service\" : \"United States Secret Service\"}\n",
    "    # Fix ambiguous topics\n",
    "    domestic_mapping_df = fix_topics(domestic_mapping_df, domestic_id_replacement, domestic_name_replacement, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MAPPING:\n",
    "    all_mappings = [terrorism_mapping_df, domestic_mapping_df, top_30_terrorism_mapping_df]\n",
    "    # Concatenate all mappings into one dataframe\n",
    "    all_mappings_df = pd.concat(all_mappings).reset_index(drop=True)\n",
    "    # Save dataframe to pickle\n",
    "    all_mappings_df.to_pickle(data_path+\"mapping.pkl\")"
   ]
  },
  {
   "source": [
    "### Step 2: Create dataframes from found topics and generated google trends anchor banks"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(domain_name, geo, t):\n",
    "    \"\"\"Creates a dataframe concatenating all search queries interest over time data from google\n",
    "    trends for one domain. The search queries will use the corresponding topics to the DHS category for the domain.\n",
    "    Prints parametters of queries who failed.\n",
    "    The returned dataframe has attributes:\n",
    "    {date, max_ratio, max_ratio_hi, max_ratio_l, article_name, topic_name, topic_id, geo}\n",
    "\n",
    "    Args:\n",
    "        domain_name (str): domain of the search queries (ie. terrorims, domestic, top_30_terrorism)       \n",
    "        geo (str): geolocalisation of the search query\n",
    "        t (GTAB): GoogleTrendsAnchorBank to use for the queries it needs to be consistent with the geo parameter\n",
    "\n",
    "    Returns:\n",
    "        dataframe: dataframe concatenating all search queries interest over time data from google\n",
    "    trends\n",
    "\n",
    "    \"\"\"\n",
    "    with open(data_path+\"mapping.pkl\", \"rb\") as file:\n",
    "        mapping_df = pickle.load(file)\n",
    "    \n",
    "    # Get the list of the article names of this domain \n",
    "    topic_queries_articles = mapping_df[mapping_df[\"domain_name\"] == domain_name][\"search_term\"].tolist()\n",
    "    # Get the list of topic ids of this domain \n",
    "    topic_queries_ids = mapping_df[mapping_df[\"domain_name\"] == domain_name][\"entity_id\"].tolist()\n",
    "    # Get the list of topic names of this domain \n",
    "    topic_queries_name = mapping_df[mapping_df[\"domain_name\"] == domain_name][\"entity_name\"].tolist()\n",
    "    \n",
    "    # For each search query freebase id create the corresponding interest over time google trends data\n",
    "    all_interest_over_time_dfs = [t.new_query(search_query) for search_query in topic_queries_ids]\n",
    "    \n",
    "    # Find all queries who succeeded                 \n",
    "    successful_queries = [type(df) != type(-1) for df in all_interest_over_time_dfs]\n",
    "    \n",
    "    df_to_concatenate = []\n",
    "    # Append the name and location to all dataframes\n",
    "    for i, df in enumerate(all_interest_over_time_dfs):\n",
    "        \n",
    "        # Was it a successful query\n",
    "        if successful_queries[i]:\n",
    "            \n",
    "            # Add the article name collumn \n",
    "            df[\"article_name\"] = [topic_queries_articles[i]]*len(df)\n",
    "            # Add the topic name collumn \n",
    "            df[\"topic_id\"] = [topic_queries_ids[i]]*len(df)\n",
    "            # Add the topic id collumn \n",
    "            df[\"topic_name\"] = [topic_queries_name[i]]*len(df)\n",
    "            # Add the localisation collumn\n",
    "            df[\"geo\"] = [\"worldwide\" if geo == \"\" else geo]*len(df)\n",
    "            \n",
    "            df_to_concatenate.append(df)\n",
    "            \n",
    "        # If not we print the parametters that failed\n",
    "        else:\n",
    "            print(\"Found a failed query\")\n",
    "            print(f\"Article name: {topic_queries_articles[i]}\")\n",
    "            print(f\"Topic name: {topic_queries_name[i]}\")\n",
    "            print(f\"Topic id: {topic_queries_ids[i]}\")\n",
    "    \n",
    "    # Concatenate all dfs into one centrale one\n",
    "    return pd.concat(df_to_concatenate).reset_index()\n",
    "    \n",
    "def get_topics(domain_name, data_path):\n",
    "    \"\"\"Gives the list of all topics corresponding to the search queries for the specific domain.\n",
    "    \n",
    "    Args:\n",
    "         domain_name (str): domain name of the search queries (i.e: terrorism, domestic, top_30_terrorism)\n",
    "         data_path (str): directory path containing the mapping pickle file \n",
    "    \n",
    "    Returns:\n",
    "        list of all topics for this domain\n",
    "    \"\"\"\n",
    "    \n",
    "    mapping_df = pd.read_pickle(data_path+\"mapping.pkl\")\n",
    "    \n",
    "    return mapping_df[mapping_df[\"domain_name\"] == domain_name][\"entity_id\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE_DATAFRAMES = False\n",
    "if CREATE_DATAFRAMES:\n",
    "    # We choose the US geolocalisation because categories are from the DHS\n",
    "    for geo in [\"US\", \"CA\", \"GB\", \"AU\", \"CH\"]:\n",
    "\n",
    "        # Create time frame corresponding to the paper study\n",
    "        start_timeframe = \"2012-01-01\"\n",
    "        end_timeframe   = \"2014-08-31\"\n",
    "\n",
    "        t = create_and_set_gtab(start_timeframe, end_timeframe, geo)\n",
    "\n",
    "        # Create terrorism dataframe\n",
    "        terrorism_df = create_dataframe(\"terrorism\", geo, t)\n",
    "        # Create domestic dataframe\n",
    "        domestic_df = create_dataframe(\"domestic\", geo, t)\n",
    "        # Create top-30 terrorism dataframe\n",
    "        top_30_terrorism_df = create_dataframe(\"top_30_terrorism\", geo, t)\n",
    "        # Save dataframes to pickle\n",
    "        terrorism_df.to_pickle(data_path+f\"terrorism_{geo}.pkl\")\n",
    "        domestic_df.to_pickle(data_path+f\"domestic_{geo}.pkl\")\n",
    "        top_30_terrorism_df.to_pickle(data_path+f\"top_30_terrorism_{geo}.pkl\")\n",
    "        \n",
    "        # ---------------------------------------------------------------\n",
    "\n",
    "        # Set start to paper start timeframe and end to present time (Stop at November 2020)\n",
    "        start_timeframe = \"2012-01-01\"\n",
    "        end_timeframe   = \"2020-11-30\"\n",
    "\n",
    "        t = create_and_set_gtab(start_timeframe, end_timeframe, geo)\n",
    "\n",
    "        # Create terrorism dataframe\n",
    "        terrorism_df = create_dataframe(\"terrorism\", geo, t)\n",
    "        # Create domestic dataframe\n",
    "        domestic_df = create_dataframe(\"domestic\", geo, t)\n",
    "        # Create top-30 terrorism dataframe\n",
    "        top_30_terrorism_df = create_dataframe(\"top_30_terrorism\", geo, t)\n",
    "        # Save dataframe to pickle\n",
    "        terrorism_df.to_pickle(data_path+f\"terrorism_present_{geo}.pkl\")\n",
    "        domestic_df.to_pickle(data_path+f\"domestic_present_{geo}.pkl\")\n",
    "        top_30_terrorism_df.to_pickle(data_path+f\"top_30_terrorism_present_{geo}.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Replicate the experiment of the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/'\n",
    "\n",
    "if path.exists(data_path):\n",
    "    with open(data_path+\"terrorism_US.pkl\", \"rb\") as f:\n",
    "        terrorism_df = pickle.load(f)\n",
    "    with open(data_path+\"domestic_US.pkl\", \"rb\") as f:\n",
    "        domestic_df = pickle.load(f)\n",
    "    with open(data_path+\"top_30_terrorism_US.pkl\", \"rb\") as f:\n",
    "        top_30_terrorism_df = pickle.load(f)\n",
    "else:\n",
    "    raise NotADirectoryError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terrorism_df = terrorism_df[['date', 'max_ratio', 'topic_name']]\n",
    "print(terrorism_df.shape)\n",
    "terrorism_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a constant for the date of revelation\n",
    "revelation_date = pd.Timestamp('2013-06-15')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organise the data in a time series with the ratio of total views per week\n",
    "terrorism_ts = terrorism_df.groupby(by='date').sum()\n",
    "terrorism_ts = terrorism_ts.reset_index().rename(columns={'index': 'date'})\n",
    "\n",
    "# Separate the data in two to perform an ITS analysis\n",
    "terrorism_pre_june_ts = terrorism_ts[terrorism_ts['date'] < revelation_date]\n",
    "terrorism_post_june_ts = terrorism_ts[terrorism_ts['date'] > revelation_date]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(terrorism_pre_june_ts['max_ratio']))\n",
    "print(np.mean(terrorism_post_june_ts['max_ratio']))\n",
    "print(terrorism_post_june_ts.shape)\n",
    "terrorism_pre_june_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_pre_june = np.mean(terrorism_pre_june_ts['max_ratio'])\n",
    "mean_post_june = np.mean(terrorism_post_june_ts['max_ratio'])\n",
    "plt.bar([0, 1], [mean_pre_june, mean_post_june], color='gray', width=0.4)\n",
    "plt.xticks([0, 1], ['Pre June, 2013', 'Post June, 2013'])\n",
    "plt.ylabel(\"Average interest\")\n",
    "plt.xlim(-0.5, 1.5)\n",
    "plt.ylim(0, 200)\n",
    "plt.text(-0.1, mean_pre_june + 5, f\"{mean_pre_june:.2f}\")\n",
    "plt.text(0.9, mean_post_june + 5, f\"{mean_post_june:.2f}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revelation_date_ord = revelation_date.toordinal()\n",
    "terrorism_pre_june_ts[\"ord_date\"] = terrorism_pre_june_ts.date.apply(lambda x: int((x.toordinal() - terrorism_pre_june_ts.date[0].toordinal())/7))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO Modify description of the plot\n",
    "\n",
    "# Visualisation of full time series\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.set_title('Figure 2. Pre and Post June 2013 Topic Ratio Trends \\n\\n', fontsize=14)\n",
    "ax.set_xlabel('\\nTime (?Dates?)', fontsize=11)\n",
    "ax.set_ylabel('Total views (All ?48? Google Articles)\\n', fontsize=11)\n",
    "\n",
    "ax.scatter(terrorism_ts.date, terrorism_ts['max_ratio'], \n",
    "    label='Total Articles Views (By Month)', color='black')\n",
    "\n",
    "# Draw the regressions\n",
    "slope_pre_june, intercept_pre_june, _, _, _ = stats.linregress(terrorism_pre_june_ts.index, \n",
    "                                                               terrorism_pre_june_ts['max_ratio'])\n",
    "slope_post_june, intercept_post_june, _, _, _ = stats.linregress(terrorism_post_june_ts.index,\n",
    "                                                                 terrorism_post_june_ts['max_ratio'])\n",
    "x1 = terrorism_pre_june_ts.index\n",
    "ax.plot(terrorism_pre_june_ts.date, intercept_pre_june + slope_pre_june*x1, \n",
    "    color='black', linewidth='3', label='Trend Pre-June 2013')\n",
    "x2 = terrorism_post_june_ts.index\n",
    "ax.plot(terrorism_post_june_ts.date, intercept_post_june + slope_post_june*x2, \n",
    "    color='gray', linewidth='3', label='Trend Post-June 2013')\n",
    "\n",
    "# Emphasise when the revelations occured \n",
    "ax.axvline(x=revelation_date, ymin=0, ymax=280, color='black', linewidth='3')\n",
    "\n",
    "x_text_offset = revelation_date - pd.Timedelta(50, 'd')\n",
    "y_text_offset = 305\n",
    "ax.text(x_text_offset, y_text_offset, 'Mid June 2013', fontsize=10)\n",
    "\n",
    "ax.set_ylim(0, 300)\n",
    "\n",
    "ax.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3 - Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}